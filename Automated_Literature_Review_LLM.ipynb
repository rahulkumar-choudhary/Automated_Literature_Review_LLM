{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "import spacy   \n",
    "from nltk.corpus import stopwords \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = load_dataset(\"allenai/scitldr\", \"AIC\")\n",
    "\n",
    "train_dataset = load_dataset(\"allenai/scitldr\", \"AIC\", split=\"train\")\n",
    "valid_dataset = load_dataset(\"allenai/scitldr\", \"AIC\", split=\"validation\")\n",
    "test_dataset  = load_dataset(\"allenai/scitldr\", \"AIC\", split=\"test\")\n",
    "\n",
    "train_df = train_dataset.to_pandas()\n",
    "valid_df = valid_dataset.to_pandas()\n",
    "test_df = test_dataset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Dataset Shape:', train_df.shape)\n",
    "print('Validation Dataset Shape:', valid_df.shape)\n",
    "print('Test Dataset Shape:', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df[\"source\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"source\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train = train_df[\"source\"].apply(lambda x: \" \".join(x))\n",
    "target_train = train_df[\"target\"].apply(lambda x: \" \".join(x))\n",
    "source_valid = valid_df[\"source\"].apply(lambda x: \" \".join(x))\n",
    "target_valid = valid_df[\"target\"].apply(lambda x: \" \".join(x))\n",
    "source_test = test_df[\"source\"].apply(lambda x: \" \".join(x))\n",
    "target_test = test_df[\"target\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({'source': source_train, 'target': target_train})\n",
    "df_valid = pd.DataFrame({'source': source_valid, 'target': target_valid})\n",
    "df_test = pd.DataFrame({'source': source_test, 'target': target_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[.*?\\]', '', text) \n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stop_words]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Preprocessing Training Data\")\n",
    "clean_train_source = source_train.progress_apply(preprocess_text)\n",
    "clean_train_target = target_train.progress_apply(preprocess_text)\n",
    "tqdm.pandas(desc=\"Preprocessing Validation Data\")\n",
    "clean_valid_source = source_valid.progress_apply(preprocess_text)\n",
    "clean_valid_target = target_valid.progress_apply(preprocess_text)\n",
    "tqdm.pandas(desc=\"Preprocessing Testing Data\")\n",
    "clean_test_source = source_test.progress_apply(preprocess_text)\n",
    "clean_test_target = target_test.progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_length_distribution(before, after, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.histplot(before.str.split().apply(len), bins=30, kde=True, label=\"Before\", color=\"blue\")\n",
    "    sns.histplot(after.str.split().apply(len), bins=30, kde=True, label=\"After\", color=\"red\")\n",
    "    plt.xlabel(\"Number of Words\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_text_length_distribution(source_train, clean_train_source, \"Text Length Distribution (Train Source)\")\n",
    "plot_text_length_distribution(target_train, clean_train_target, \"Text Length Distribution (Train Target)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(texts, title):\n",
    "    text = \" \".join(texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_wordcloud(source_train, \"Word Cloud Before Cleaning (Train Source)\")\n",
    "plot_wordcloud(clean_train_source, \"Word Cloud After Cleaning (Train Source)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_common_words(texts, title, n=20):\n",
    "    words = \" \".join(texts).split()\n",
    "    word_counts = Counter(words)\n",
    "    most_common_words = word_counts.most_common(n)\n",
    "    words, counts = zip(*most_common_words)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(x=list(words), y=list(counts), palette=\"viridis\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_most_common_words(source_train, \"Most Common Words Before Cleaning (Train Source)\")\n",
    "plot_most_common_words(clean_train_source, \"Most Common Words After Cleaning (Train Source)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_embeddings = sbert_model.encode(batch, convert_to_tensor=True).cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "source_train_embeddings = generate_embeddings(clean_train_source.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS (Facebook AI Similarity Search) for efficiently search for similar text embeddings.\n",
    "import faiss\n",
    "\n",
    "# Convert embeddings to a NumPy array\n",
    "source_train_embeddings_np = np.array(source_train_embeddings).astype(\"float32\")\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(source_train_embeddings_np.shape[1])  # L2 (Euclidean) distance\n",
    "index.add(source_train_embeddings_np)  # Add embeddings to the index\n",
    "\n",
    "# Example: Search for the 5 most similar documents to the first document\n",
    "query_embedding = source_train_embeddings_np[0].reshape(1, -1)  # Querying the first document\n",
    "D, I = index.search(query_embedding, 5)  # Returns distances (D) and indices (I)\n",
    "\n",
    "print(\"Top 5 similar document indices:\", I)\n",
    "print(\"Top 5 similarity scores:\", D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 (Text-to-Text Transfer Transformer) used to generate summaries from source text.\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def summarize_text(text, max_length=50):\n",
    "    input_text = \"summarize: \" + text  # Prefix needed for T5\n",
    "    input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = t5_model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example: Summarizing the first document\n",
    "example_summary = summarize_text(source_train[0])\n",
    "print(\"Original Text:\", source_train[0])\n",
    "print(\"Summarized Text:\", example_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic and LDA are used for topic modeling and document clustering.\n",
    "\n",
    "# Implementation using BERTopic:\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Initialize and fit BERTopic model\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(clean_train_source.tolist(), source_train_embeddings)\n",
    "\n",
    "# Display the topics\n",
    "topic_model.get_topic_info().head(10)\n",
    "\n",
    "# Visualize the topics\n",
    "topic_model.visualize_barchart(top_n_topics=10)\n",
    "\n",
    "# Implementation using LDA (Latent Dirichlet Allocation):\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Tokenize the cleaned source text\n",
    "tokenized_texts = [text.split() for text in clean_train_source.tolist()]\n",
    "\n",
    "# Create a dictionary and corpus\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
    "\n",
    "# Display top words for each topic\n",
    "for idx, topic in lda_model.show_topics(formatted=True, num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FAISS for fast similarity search over the source document embeddings.\n",
    "\n",
    "# Use T5 to generate summaries from documents.\n",
    "\n",
    "# Use BERTopic and LDA for topic modeling and document clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
